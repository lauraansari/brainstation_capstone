{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Notebook 05 - Decision tree models**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction\n",
    "\n",
    "In the previous notebook, I optimized and fitted two linear regression models, and evaluated how they do on the train and test sets. In this section, I'll explore decision tree-based models to see if they improve prediction accuracy of future AQI values. I'll compare these model(s) to the ones already built in Notebook 3 and 4. For this, I'll use the dataset with the lag features from the previous notebook as opposed to the original dataset.\n",
    "\n",
    "A brief summary is provided at the end of the notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Table of content:**\n",
    "\n",
    "\n",
    "- Loading data and importing libraries\n",
    "\n",
    "- Data preprocessing:\n",
    "\n",
    "    - Train-test split\n",
    "\n",
    "- Modeling:\n",
    "\n",
    "    - Decision-tree model\n",
    "    - XGBoost\n",
    "        - Hyperparameter optimization for XGBoost\n",
    "- Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "#### Loading data and importing libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing libraries\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt \n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "from plotly.subplots import make_subplots\n",
    "import plotly.graph_objs as go\n",
    "from statsmodels.api import tsa\n",
    "import statsmodels.api as sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>AQI</th>\n",
       "      <th>Category</th>\n",
       "      <th>Defining Parameter</th>\n",
       "      <th>Number of Sites Reporting</th>\n",
       "      <th>city_ascii</th>\n",
       "      <th>state_name</th>\n",
       "      <th>lat</th>\n",
       "      <th>lng</th>\n",
       "      <th>population</th>\n",
       "      <th>density</th>\n",
       "      <th>...</th>\n",
       "      <th>Year</th>\n",
       "      <th>Month</th>\n",
       "      <th>Day</th>\n",
       "      <th>ma_last_7d</th>\n",
       "      <th>ma_last_30d</th>\n",
       "      <th>ma_last_quarter</th>\n",
       "      <th>AQI_lag_1days</th>\n",
       "      <th>AQI_lag_7days</th>\n",
       "      <th>AQI_lag_30days</th>\n",
       "      <th>AQI_lag_365days</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1987-06-11</th>\n",
       "      <td>240</td>\n",
       "      <td>Very Unhealthy</td>\n",
       "      <td>Ozone</td>\n",
       "      <td>21</td>\n",
       "      <td>Los Angeles</td>\n",
       "      <td>California</td>\n",
       "      <td>34.1141</td>\n",
       "      <td>-118.4068</td>\n",
       "      <td>12531334.0</td>\n",
       "      <td>3267.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1987</td>\n",
       "      <td>6</td>\n",
       "      <td>11</td>\n",
       "      <td>200.714286</td>\n",
       "      <td>165.466667</td>\n",
       "      <td>153.428571</td>\n",
       "      <td>232.0</td>\n",
       "      <td>232.0</td>\n",
       "      <td>245.0</td>\n",
       "      <td>248.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1984-03-01</th>\n",
       "      <td>164</td>\n",
       "      <td>Unhealthy</td>\n",
       "      <td>Ozone</td>\n",
       "      <td>20</td>\n",
       "      <td>Los Angeles</td>\n",
       "      <td>California</td>\n",
       "      <td>34.1141</td>\n",
       "      <td>-118.4068</td>\n",
       "      <td>12531334.0</td>\n",
       "      <td>3267.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1984</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>112.428571</td>\n",
       "      <td>117.833333</td>\n",
       "      <td>131.439560</td>\n",
       "      <td>115.0</td>\n",
       "      <td>110.0</td>\n",
       "      <td>140.0</td>\n",
       "      <td>57.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1984-09-12</th>\n",
       "      <td>90</td>\n",
       "      <td>Moderate</td>\n",
       "      <td>Ozone</td>\n",
       "      <td>22</td>\n",
       "      <td>Los Angeles</td>\n",
       "      <td>California</td>\n",
       "      <td>34.1141</td>\n",
       "      <td>-118.4068</td>\n",
       "      <td>12531334.0</td>\n",
       "      <td>3267.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1984</td>\n",
       "      <td>9</td>\n",
       "      <td>12</td>\n",
       "      <td>169.857143</td>\n",
       "      <td>213.833333</td>\n",
       "      <td>229.000000</td>\n",
       "      <td>67.0</td>\n",
       "      <td>255.0</td>\n",
       "      <td>237.0</td>\n",
       "      <td>297.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            AQI        Category Defining Parameter  Number of Sites Reporting  \\\n",
       "Date                                                                            \n",
       "1987-06-11  240  Very Unhealthy              Ozone                         21   \n",
       "1984-03-01  164       Unhealthy              Ozone                         20   \n",
       "1984-09-12   90        Moderate              Ozone                         22   \n",
       "\n",
       "             city_ascii  state_name      lat       lng  population  density  \\\n",
       "Date                                                                          \n",
       "1987-06-11  Los Angeles  California  34.1141 -118.4068  12531334.0   3267.0   \n",
       "1984-03-01  Los Angeles  California  34.1141 -118.4068  12531334.0   3267.0   \n",
       "1984-09-12  Los Angeles  California  34.1141 -118.4068  12531334.0   3267.0   \n",
       "\n",
       "            ...  Year  Month  Day  ma_last_7d  ma_last_30d  ma_last_quarter  \\\n",
       "Date        ...                                                               \n",
       "1987-06-11  ...  1987      6   11  200.714286   165.466667       153.428571   \n",
       "1984-03-01  ...  1984      3    1  112.428571   117.833333       131.439560   \n",
       "1984-09-12  ...  1984      9   12  169.857143   213.833333       229.000000   \n",
       "\n",
       "            AQI_lag_1days  AQI_lag_7days  AQI_lag_30days  AQI_lag_365days  \n",
       "Date                                                                       \n",
       "1987-06-11          232.0          232.0           245.0            248.0  \n",
       "1984-03-01          115.0          110.0           140.0             57.0  \n",
       "1984-09-12           67.0          255.0           237.0            297.0  \n",
       "\n",
       "[3 rows x 21 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Loading dataset\n",
    "\n",
    "aqi_df = pd.read_csv('data/air_quality_with_lags.csv', parse_dates=['Date'], index_col='Date')\n",
    "aqi_df.sample(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "#### Pre-processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since I already did some feature-engineering for the linear models and loaded the dataset that includes these lag and time features, I won't need to perform further feature engineering for the notebook. Before modeling, I'll scale and split the dataset into remainder and test sets. This time I won't be splitting it down any further, since I'll use cross-validation to optimize the models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting X and y variables\n",
    "\n",
    "X = aqi_df.drop(columns=['AQI', 'Category', 'Defining Parameter', 'Number of Sites Reporting', 'city_ascii', 'state_name', 'lat', 'lng', 'population', 'density', 'timezone'])\n",
    "y = aqi_df['AQI']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train-test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train-test split into test and remainder sets to set aside some unseen data\n",
    "\n",
    "X_remainder = pd.DataFrame(X.loc[X.index <= '2013-01-01'])\n",
    "y_remainder = pd.DataFrame(y.loc[y.index <= '2013-01-01'])\n",
    "\n",
    "X_test = pd.DataFrame(X.loc[X.index > '2013-01-01'])\n",
    "y_test = pd.DataFrame(y.loc[y.index > '2013-01-01'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Scaling the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scaling the data\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_remainder_s = scaler.fit_transform(X_remainder)\n",
    "X_test_s = scaler.transform(X_test)\n",
    "\n",
    "X_remainder = pd.DataFrame(data=X_remainder_s, columns=X_remainder.columns, index=X_remainder.index)\n",
    "X_test = pd.DataFrame(data=X_test_s, columns=X_test.columns, index=X_test.index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Decision tree model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reminder on the results of the baseline model:\n",
    "\n",
    "| Model  |  Train results |  Test results |\n",
    "|---|---|---|\n",
    "|  Baseline (mean prediction) | 99.39%  | 100.31%  |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decision tree model\n",
    "\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "tree = DecisionTreeRegressor(max_depth=1)\n",
    "tree.fit(X_remainder, y_remainder);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_train = tree.predict(X_remainder).reshape(11690,1)\n",
    "preds_test = tree.predict(X_test).reshape(3286,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36.52\n",
      "44.16\n"
     ]
    }
   ],
   "source": [
    "mape_train = round(np.mean(abs((y_remainder - preds_train) / y_remainder)) * 100, 2)\n",
    "mape_test = round(np.mean(abs((y_test - preds_test) / y_test)) * 100, 2)\n",
    "\n",
    "print(mape_train)\n",
    "print(mape_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the model is overfitting, it's error percentage is much lower on the training dataset than on the test set.\n",
    "\n",
    "Let's add these results to the table.\n",
    "\n",
    "| Model  |  Train results |  Test results |\n",
    "|---|---|---|\n",
    "|  Baseline (mean prediction) | 99.39%  | 100.31%  |\n",
    "|  Decision Tree | 36.52%  | 44.16%  |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the model does much better than the baseline model, however, the error percentage is still quite big. Instead of further optimizing the decision tree model, I'll move on to build a more robust decision tree-based model. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "### XGBoost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First let's see how the model does with the default parameters without modifying or optimizing any of them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Building the first XGBoost model without hyperparameter optimization\n",
    "\n",
    "xgb = XGBRegressor()\n",
    "xgb.fit(X_remainder, y_remainder)\n",
    "\n",
    "# Predictions on the training and test sets\n",
    "\n",
    "preds_xgb_train = xgb.predict(X_remainder).reshape(11690,1)\n",
    "preds_xgb_test = xgb.predict(X_test).reshape(3286,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15.49\n",
      "27.33\n"
     ]
    }
   ],
   "source": [
    "# Extracting MAPE scores\n",
    "\n",
    "mape_train = round(np.mean(abs((y_remainder - preds_xgb_train) / y_remainder)) * 100, 2)\n",
    "mape_test = round(np.mean(abs((y_test - preds_xgb_test) / y_test)) * 100, 2)\n",
    "\n",
    "print(mape_train)\n",
    "print(mape_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results are generally much better compared to the decision tree model we've just seen. The XGBoost model does a great job on predicting the target variable on the training set, however, it does a less decent job on the test set. It's heavily overfitting and the error % is still quite high on the test set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Model  |  Train results |  Test results |\n",
    "|---|---|---|\n",
    "|  Baseline (mean prediction) | 99.39%  | 100.31%  |\n",
    "|  Decision Tree (max depth = 1) | 36.52%  | 44.16%  |\n",
    "|  XGBoost() | 15.49%  | 27.33%  |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To improve these results, I'll perform a grid-search using 3-fold validation. I'll optimize the following parameters:\n",
    "\n",
    "- `eta` (or learning rate): the default values is 0.3, but the previous model was heavily overfitting, so I'll try something smaller\n",
    "- `max_depth`: adding more depth to the model makes is more complex, and more lilely to overfit, so I'll try something in the lower ranges first\n",
    "- `n_estimators`: controls the number of trees in the model, increasing this value might lead to overfitting, so I'll try something between 500-700\n",
    "\n",
    "Source: https://medium.com/@rithpansanga/the-main-parameters-in-xgboost-and-their-effects-on-model-performance-4f9833cac7c "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hyperparameter optimization for XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grid search for XGBoost\n",
    "\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.model_selection import TimeSeriesSplit, GridSearchCV\n",
    "\n",
    "xgb = XGBRegressor(eval_metric='mape')\n",
    "\n",
    "max_depth = [6, 7, 8]\n",
    "estimators = [500, 600, 700]\n",
    "rate = [0.01, 0.1, 0.2]\n",
    "\n",
    "my_param_grid = ({'max_depth': max_depth,\n",
    "                 'n_estimators': estimators,\n",
    "                 'eta': rate})\n",
    "\n",
    "# Cross-validation split for time-series data\n",
    "\n",
    "my_cv = TimeSeriesSplit(n_splits=3).split(X_remainder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 27 candidates, totalling 81 fits\n"
     ]
    }
   ],
   "source": [
    "# Instantiate the xgboost grid search\n",
    "\n",
    "gridsearch = GridSearchCV(xgb, param_grid=my_param_grid, cv=my_cv, verbose=1)\n",
    "\n",
    "# Fit the grid search\n",
    "\n",
    "fitted_xgb = gridsearch.fit(X_remainder, y_remainder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'eta': 0.01, 'max_depth': 6, 'n_estimators': 500}"
      ]
     },
     "execution_count": 202,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Best parameters\n",
    "\n",
    "fitted_xgb.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the best parameters were all the smallest possible values I added earlier, so it's worth running another grid search with smaller values - this is to see if smaller values for the parameters improve the performance or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grid search for XGBoost\n",
    "\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.model_selection import TimeSeriesSplit, GridSearchCV\n",
    "\n",
    "xgb = XGBRegressor()\n",
    "\n",
    "max_depth = [4, 5, 6]\n",
    "estimators = [200, 300, 400, 500]\n",
    "rate = [0.005, 0.008, 0.01]\n",
    "eval_met = ['mape', 'rmse']\n",
    "\n",
    "my_param_grid = ({'max_depth': max_depth,\n",
    "                 'n_estimators': estimators,\n",
    "                 'learning_rate': rate,\n",
    "                 'eval_metric': eval_met})\n",
    "\n",
    "# Cross-validation split for time-series data\n",
    "\n",
    "my_cv = TimeSeriesSplit(n_splits=3).split(X_remainder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 72 candidates, totalling 216 fits\n"
     ]
    }
   ],
   "source": [
    "# Instantiate the log reg grid search\n",
    "\n",
    "gridsearch = GridSearchCV(xgb, param_grid=my_param_grid, cv=my_cv, verbose=1)\n",
    "\n",
    "# Fit the log reg grid search\n",
    "\n",
    "fitted_xgb = gridsearch.fit(X_remainder, y_remainder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'eval_metric': 'mape',\n",
       " 'learning_rate': 0.01,\n",
       " 'max_depth': 4,\n",
       " 'n_estimators': 500}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Best parameters\n",
    "\n",
    "fitted_xgb.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the `learning rate` and `n_estimators`, the grid search still picks up the same parameters, however, the `maximum depth` is now 3. Since this is a middle value in our range, I can be confident that no bigger or smaller value will improve the model further. Now let's build the model using these parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Building the model with the best performing parameters\n",
    "\n",
    "xgb = XGBRegressor(max_depth=4, n_estimators=500, learning_rate=0.01)\n",
    "xgb.fit(X_remainder, y_remainder)\n",
    "\n",
    "preds_xgb_train = xgb.predict(X_remainder).reshape(11690,1)\n",
    "preds_xgb_test = xgb.predict(X_test).reshape(3286,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23.51\n",
      "20.89\n"
     ]
    }
   ],
   "source": [
    "# Extracting MAPE scores\n",
    "\n",
    "mape_train = round(np.mean(abs((y_remainder - preds_xgb_train) / y_remainder)) * 100, 2)\n",
    "mape_test = round(np.mean(abs((y_test - preds_xgb_test) / y_test)) * 100, 2)\n",
    "\n",
    "print(mape_train)\n",
    "print(mape_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model is not overfitting anymore, and the results improved a lot compared to the previous models. The test results are again slightly better than the train scores, however, as per the previous notebook, I'll assume this is because of the nature of the dataset.\n",
    "\n",
    "Summary of the decision-tree based models' performance:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "bat"
    }
   },
   "source": [
    "| Model  |  Train results |  Test results |\n",
    "|---|---|---|\n",
    "|  Baseline (mean prediction) | 99.39%  | 100.31%  |\n",
    "|  Decision Tree (max depth = 1) | 36.52%  | 44.16%  |\n",
    "|  XGBoost() | 15.49%  | 27.33%  |\n",
    "|  XGBoost (max_depth=4, n_estimators=500, eta=0.01) | 23.51%  | 20.89%  |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "### Brief summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The decision-tree model without optimization didn't do particularly well on the dataset, however, it was an improvement over the baseline model\n",
    "- The first **XGBoost model** without hyperparameter optimization was **heavily overfitting** on the training data\n",
    "- We managed to overcome the overfitting, and found the **best-performing XGBoost model through Grid search**\n",
    "- However, it didn't do better than the linear models or the auto-regressive models, with a final **20.98% MAPE score over the test set**\n",
    "- XGboost models are also quite difficult to interpret, which needs to be considered when picking the final model. Since they didn't overdo the previously models, I won't be going forward with this model for the final forecasting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Summary table of the best-performing models:\n",
    "\n",
    "| Model  |  Train results |  Test results |\n",
    "|---|---|---|\n",
    "|  Baseline (mean prediction) | 99.39%  | 100.31%  |\n",
    "|  SARIMA(1,1,2)(1, 1, 1, 12) | 13.46%  | 12.18%  |\n",
    "|  Linear regression | 24.36%  | 19.75%  |\n",
    "|  XGBoost (max_depth=4, n_estimators=500, eta=0.01) | 23.51%  | 20.89%  |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***In the next section, I'll use the best-performing model (SARIMA) to forecast future AQI values and save the final model.***"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "capstone",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
